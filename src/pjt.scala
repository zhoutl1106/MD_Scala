/**
  * Created by zhoutianli on 4/21/16.
  */

import java.util

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}
import org.apache.spark.rdd.RDD
import org.apache.log4j.{Level, Logger}

import scala.collection.mutable.Map
import scala.collection.mutable.ArrayBuffer
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.parquet.it.unimi.dsi.fastutil.Arrays

import scala.io.Source

//object Hdfs extends App {
//
//  def write(uri: String, filePath: String, data: Array[Byte]) = {
//    System.setProperty("HADOOP_USER_NAME", "tzhou8")
//    val path = new Path(filePath)
//    val conf = new Configuration()
//    conf.set("fs.defaultFS", uri)
//    val fs = FileSystem.get(conf)
//    val os = fs.create(path)
//    os.write(data)
//    fs.close()
//  }
//}

object pjt {
  def main(args: Array[String]): Unit = {

    val cluster = args(0).toInt > 0
    val threads = 4
    val result = ArrayBuffer.empty[String]
    val conf = new SparkConf().setAppName("CS526, HW2, Tianli ZHOU")
    if (cluster) {
      // Setup SparkContext for Cluster mode
      conf.setMaster("yarn-cluster")
      conf.set("spark.executor.memory", "64g")
      conf.set("spark.eventLog.dir", "hdfs://beacon040-ib0/user/tzhou8/events/")
    }
    else {
      // Setup SparkContext for Local mode
      conf.setMaster("local[" + threads + "]").set("spark.executor.memory", "6g")
    }
    val sc = new SparkContext(conf)

//    var rawData = sc.parallelize[String](Seq("", ""))
//    if (cluster) {
//      if (args.length > 1)
//        rawData = sc.textFile(args(1)).cache()
//      else
//        rawData = sc.textFile("hdfs://beacon040-ib0/user/tzhou8/pjt/1000.xyz").cache()
//    }
//    else {
//      if (args.length > 1)
//        rawData = sc.textFile(args(1)).cache()
//      else
//        rawData = sc.textFile("/Users/zhoutianli/Classes/CS526/Simulations/1000.xyz")
//    }
    //
    //        val rawSplit = rawData.map(_.split(" +").filterNot(_ == ""))
    //      .filter(_.length > 3).cache()
    //    val data = rawSplit.map(p=>{
    //      if(p.length > 3)
    //        (p(0),p(1).toDouble,p(2).toDouble,p(3).toDouble)
    //    })

    //    val rawSpilt = rawData.map(_.split("(?sim)\\.*944 generated by VMD.*")  ).cache()
    //    println(rawSpilt.count())
    //    println(rawSpilt.collect()(0))//    val arr = Array(1.0,2.0,3.0,4.0,5.0,6.0)

    val md = MD_Scala
        md.Set_Spark_Context(sc)
    if (args.length > 1)
      md.Load_XYZ_File("944\n generated by VMD\n",
        args(1))
    else
      md.Load_XYZ_File("944\n generated by VMD\n",
        "/Users/zhoutianli/Classes/CS526/Simulations/1000.xyz")

    var loop = 99
    if(args.length > 2)
      loop = args(2).toInt

    val abs = Array.ofDim[Double](loop+1,loop+1)

    val err = Array.ofDim[Double](loop+1,loop+1)

    (0 to loop).map(i=>{
      (0 to loop).map(j=>{
        if(i != j)
        {
          md.Set_Cluster(false)
          val vv1 = md.Get_Kurtosis_of_Pair(i, j, 0)
//          md.Set_Cluster(true)
//          val vv2 = md.Get_Kurtosis_of_Pair(i, j, 0)
          abs(i)(j) = vv1
//          err(i)(j) = (vv1-vv2)
        }
      })
    })

    println("******")
    for(i <- 0 to loop) {
      for (j <- 0 to loop) {
        print(abs(i)(j) + " ")
      }
      println()
    }

/*
    val vv1 = md.Get_Kurtosis_of_Pair(0, 1, 1)
    val tt1: Array[Double] = Array(0.0, 0.0, 0.0, 0.0)
    tt1(0) = md.lastRunningTime(0)
    tt1(1) = md.lastRunningTime(1)
    tt1(2) = md.lastRunningTime(2)
    tt1(3) = md.lastRunningTime(3)

    val vv3 = md.Get_Kurtosis_of_Pair(0, 1, 0)
    val tt3: Array[Double] = Array(0.0, 0.0, 0.0, 0.0)
    tt3(0) = md.lastRunningTime(0)
    tt3(1) = md.lastRunningTime(1)
    tt3(2) = md.lastRunningTime(2)
    tt3(3) = md.lastRunningTime(3)

    md.Set_Cluster(true)
    val vv2 = md.Get_Kurtosis_of_Pair(0, 1, 1)
    val tt2: Array[Double] = Array(0.0, 0.0, 0.0, 0.0)
    tt2(0) = md.lastRunningTime(0)
    tt2(1) = md.lastRunningTime(1)
    tt2(2) = md.lastRunningTime(2)
    tt2(3) = md.lastRunningTime(3)

    val vv4 = md.Get_Kurtosis_of_Pair(0, 1, 0)
    val tt4: Array[Double] = Array(0.0, 0.0, 0.0, 0.0)
    tt4(0) = md.lastRunningTime(0)
    tt4(1) = md.lastRunningTime(1)
    tt4(2) = md.lastRunningTime(2)
    tt4(3) = md.lastRunningTime(3)

    println(md.dat.length)
    print("v1 = " + vv1 + " : ")
    tt1.foreach(e => {
      print(e + " ")
    })
    println(" ms")

    print("v2 = " + vv2 + " : ")
    tt2.foreach(e => {
      print(e + " ")
    })
    println(" ms")

    print("v3 = " + vv3 + " : ")
    tt3.foreach(e => {
      print(e + " ")
    })
    println(" ms")

    print("v4 = " + vv4 + " : ")
    tt4.foreach(e => {
      print(e + " ")
    })
    println(" ms")*/



    //println(md.dat.length + "\t" + tt1.sum + "\t" + tt2.sum + "\t" + tt3.sum + "\t" + tt4.sum)

    //

    //    println(data.count())
    //data.foreach(println)

    //    val c = rawSplit.map(p => p(index).toDouble)


    sc.stop()
  }
}